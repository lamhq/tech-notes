# Cache

## Overview

A cache is a temporary storage area that stores the result of expensive responses or frequently accessed data in memory so that subsequent requests are served more quickly.

The goal of cache is to speed up getting some data from a slower storage by storing part of this data in a faster storage.

You want to maximize cache hit ratio. Otherwise, it may introduce additional latency.

Work best for read-heavy workloads, when data is read frequently but modified infrequently.

Pros:
- Improve read performance. Reduce latency
- Increase throughtput. System can handle more requests.

Cons:
- Increase complexity to handle cache
- Introduce inconsistencies where cache is not synced with data storage


## Best Practices

Don't use cache to store critical data.

> Since cached data is stored in memory, if a cache server restarts, all the data in memory is lost.

Choose an approriate expiration time (TTL).

> Setting the expiration date too short will cause the system to reload data from the database frequently. Meanwhile, if set too long, the data can become stale.

Keep data store and cache in sync to ensure data consistency.

Set up multiple cache servers across different data centers to avoid **SPOF** (Single point of failure).

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Single_Point_of_Failure.png/400px-Single_Point_of_Failure.png)

**Overprovision the required memory for cache servers** to handle unexpected spikes in demand without performance degradation.

**Choose an approriate eviction policy** to determines which data should be removed once the cache is full.
